{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic speech transcription using Whisper\n",
    "In this module, we use [Whisper](https://github.com/openai/whisper) from OpenAI to transcribe speech automatically. Whisper is a robust automatic speech recognition (ASR) model that supports about 100 different languages (e.g., English, Italian, Dutch, Japanese, Chinese, Spanish, etc).\n",
    "\n",
    "Whisper provides 5 multilingual model sizes as follows:\n",
    "\n",
    "|  Size  | Parameters | English-only model | Multilingual model | Required VRAM | Relative speed |\n",
    "|:------:|:----------:|:------------------:|:------------------:|:-------------:|:--------------:|\n",
    "|  tiny  |    39 M    |     `tiny.en`      |       `tiny`       |     ~1 GB     |      ~32x      |\n",
    "|  base  |    74 M    |     `base.en`      |       `base`       |     ~1 GB     |      ~16x      |\n",
    "| small  |   244 M    |     `small.en`     |      `small`       |     ~2 GB     |      ~6x       |\n",
    "| medium |   769 M    |    `medium.en`     |      `medium`      |     ~5 GB     |      ~2x       |\n",
    "| large  |   1550 M   |        N/A         | `large/large-v2/large-v3` |    ~10 GB     |       1x       |\n",
    "\n",
    "As you can see, the smaller the model is, the faster computational time is, with less accurate results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues in timestamps accuracy\n",
    "The original whisper models do not correctly capture silences. Rather, they just include sliences in timestamps. <br>\n",
    "For example, if an utterance was produced from 00:00:11.000 to 00:00:15.000 followed by a 2-second silence, the timestamp will be 00:00:11.000 - 00:00:17.000 instead. <br>\n",
    "\n",
    "Also, the original whisper model doesn't provide word-level timestamps.\n",
    "\n",
    "To solve this, we will use **[whisperx](https://github.com/m-bain/whisperX)**, which first transcribes speech using faster-whisper model and use a forced-alignment algorithm for improved timing accuracy and word-level timestamps.\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of the script\n",
    "\n",
    "The script performs following processes in the listed order:\n",
    "\n",
    "1. Import packages and define path for the input and output folders\n",
    "1. Load a whisper model\n",
    "1. Go through each audio/video (wav/mp4) files in the \"input\" folder and run whisper on each file\n",
    "    1. Transcribe audio/video file using faster-whisper\n",
    "    1. Get accurate timestamps using a forced alignment algorithm\n",
    "    1. Move on to the next file\n",
    "1. Call the \"export_transcript_as_tsv()\" function to export the whisper output as a tab-delimited text file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages and define paths\n",
    "\n",
    "### Import packages and define path for input and output folders\n",
    "Let's first import required packages.\n",
    "\n",
    "\n",
    "<details><summary><font color = \"mandarin\">If you haven't installed the packages, click here and follow the steps below</font></summary>\n",
    "\n",
    "1. Open terminal/anaconda prompt at the folder in which you store this notebook\n",
    "    - Mac:\n",
    "        1. go to the \"imprs_whisper_workshop\" folder\n",
    "        1. right-click the \"imprs_whisper_workshop\" folder\n",
    "        1. click \"open terminal at this folder\" <br><br>\n",
    "    - Windows:\n",
    "        1. go to the \"imprs_whisper_workshop\" folder\n",
    "        1. copy the path to the folder\n",
    "        1. open Anaconda Prompt\n",
    "        1. type cd and paste the path after a space (e.g., cd D:/users/shoakamine/imprs_whisper_workshop)\n",
    "        1. if the folder is not in the C drive, type the drive initial followed by a colon (e.g., D:):\n",
    "\n",
    "1. Create Python3.10 environment\n",
    "    ```\n",
    "    conda create --name whisperx python=3.10\n",
    "    ```\n",
    "1. Activate your conda environment\n",
    "    ```\n",
    "    conda activate whisperx\n",
    "    ```\n",
    "\n",
    "1. Install PyTorch following the instructions [here](https://pytorch.org/get-started/previous-versions/#v200)\n",
    "\n",
    "1. Run this command to install required packages: `pip install -r requirements.txt`\n",
    "\n",
    "1. Click \"select kernel\" on the top right of VS code and select \"whisperx\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[28654]: Class AVFFrameReceiver is implemented in both /usr/local/Cellar/ffmpeg/6.1.1_6/lib/libavdevice.60.3.100.dylib (0x10e510378) and /Users/shoakamine/opt/anaconda3/envs/whisperx/lib/python3.10/site-packages/av/.dylibs/libavdevice.60.1.100.dylib (0x1383870f8). One of the two will be used. Which one is undefined.\n",
      "objc[28654]: Class AVFAudioReceiver is implemented in both /usr/local/Cellar/ffmpeg/6.1.1_6/lib/libavdevice.60.3.100.dylib (0x10e5103c8) and /Users/shoakamine/opt/anaconda3/envs/whisperx/lib/python3.10/site-packages/av/.dylibs/libavdevice.60.1.100.dylib (0x138387148). One of the two will be used. Which one is undefined.\n",
      "/Users/shoakamine/opt/anaconda3/envs/whisperx/lib/python3.10/site-packages/pyannote/audio/core/io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n"
     ]
    }
   ],
   "source": [
    "import whisperx\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# import custom-made functions\n",
    "from tsv_export import export_transcript_as_tsv, export_transcript_as_tsv_textonly\n",
    "\n",
    "\n",
    "input_folder = \"../input/\"\n",
    "output_folder = \"../output/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model and export the output as tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the list of languages</summary>\n",
    "\n",
    "1.  \"en\": \"english\",\n",
    "1.  \"zh\": \"chinese\",\n",
    "1. \"de\": \"german\",\n",
    "1. \"es\": \"spanish\",\n",
    "1. \"ru\": \"russian\",\n",
    "1. \"ko\": \"korean\",\n",
    "1. \"fr\": \"french\",\n",
    "1. \"ja\": \"japanese\",\n",
    "1. \"pt\": \"portuguese\",\n",
    "1. \"tr\": \"turkish\",\n",
    "1. \"pl\": \"polish\",\n",
    "1. \"ca\": \"catalan\",\n",
    "1. \"nl\": \"dutch\",\n",
    "1. \"ar\": \"arabic\",\n",
    "1. \"sv\": \"swedish\",\n",
    "1. \"it\": \"italian\",\n",
    "1. \"id\": \"indonesian\",\n",
    "1. \"hi\": \"hindi\",\n",
    "1. \"fi\": \"finnish\",\n",
    "1. \"vi\": \"vietnamese\",\n",
    "1. \"he\": \"hebrew\",\n",
    "1. \"uk\": \"ukrainian\",\n",
    "1. \"el\": \"greek\",\n",
    "1. \"ms\": \"malay\",\n",
    "1. \"cs\": \"czech\",\n",
    "1. \"ro\": \"romanian\",\n",
    "1. \"da\": \"danish\",\n",
    "1. \"hu\": \"hungarian\",\n",
    "1. \"ta\": \"tamil\",\n",
    "1. \"no\": \"norwegian\",\n",
    "1. \"th\": \"thai\",\n",
    "1. \"ur\": \"urdu\",\n",
    "1. \"hr\": \"croatian\",\n",
    "1. \"bg\": \"bulgarian\",\n",
    "1. \"lt\": \"lithuanian\",\n",
    "1. \"la\": \"latin\",\n",
    "1. \"mi\": \"maori\",\n",
    "1. \"ml\": \"malayalam\",\n",
    "1. \"cy\": \"welsh\",\n",
    "1. \"sk\": \"slovak\",\n",
    "1. \"te\": \"telugu\",\n",
    "1. \"fa\": \"persian\",\n",
    "1. \"lv\": \"latvian\",\n",
    "1. \"bn\": \"bengali\",\n",
    "1. \"sr\": \"serbian\",\n",
    "1. \"az\": \"azerbaijani\",\n",
    "1. \"sl\": \"slovenian\",\n",
    "1. \"kn\": \"kannada\",\n",
    "1. \"et\": \"estonian\",\n",
    "1. \"mk\": \"macedonian\",\n",
    "1. \"br\": \"breton\",\n",
    "1. \"eu\": \"basque\",\n",
    "1. \"is\": \"icelandic\",\n",
    "1. \"hy\": \"armenian\",\n",
    "1. \"ne\": \"nepali\",\n",
    "1. \"mn\": \"mongolian\",\n",
    "1. \"bs\": \"bosnian\",\n",
    "1. \"kk\": \"kazakh\",\n",
    "1. \"sq\": \"albanian\",\n",
    "1. \"sw\": \"swahili\",\n",
    "1. \"gl\": \"galician\",\n",
    "1. \"mr\": \"marathi\",\n",
    "1. \"pa\": \"punjabi\",\n",
    "1. \"si\": \"sinhala\",\n",
    "1. \"km\": \"khmer\",\n",
    "1. \"sn\": \"shona\",\n",
    "1. \"yo\": \"yoruba\",\n",
    "1. \"so\": \"somali\",\n",
    "1. \"af\": \"afrikaans\",\n",
    "1. \"oc\": \"occitan\",\n",
    "1. \"ka\": \"georgian\",\n",
    "1. \"be\": \"belarusian\",\n",
    "1. \"tg\": \"tajik\",\n",
    "1. \"sd\": \"sindhi\",\n",
    "1. \"gu\": \"gujarati\",\n",
    "1. \"am\": \"amharic\",\n",
    "1. \"yi\": \"yiddish\",\n",
    "1. \"lo\": \"lao\",\n",
    "1. \"uz\": \"uzbek\",\n",
    "1. \"fo\": \"faroese\",\n",
    "1. \"ht\": \"haitian creole\",\n",
    "1. \"ps\": \"pashto\",\n",
    "1. \"tk\": \"turkmen\",\n",
    "1. \"nn\": \"nynorsk\",\n",
    "1. \"mt\": \"maltese\",\n",
    "1. \"sa\": \"sanskrit\",\n",
    "1. \"lb\": \"luxembourgish\",\n",
    "1. \"my\": \"myanmar\",\n",
    "1. \"bo\": \"tibetan\",\n",
    "1. \"tl\": \"tagalog\",\n",
    "1. \"mg\": \"malagasy\",\n",
    "1. \"as\": \"assamese\",\n",
    "1. \"tt\": \"tatar\",\n",
    "1. \"haw\": \"hawaiian\",\n",
    "1. \"ln\": \"lingala\",\n",
    "1. \"ha\": \"hausa\",\n",
    "1. \"ba\": \"bashkir\",\n",
    "1. \"jw\": \"javanese\",\n",
    "1. \"su\": \"sundanese\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Using device: cpu \n",
      "* Batch size: 4 \n",
      "* Model size: base \n",
      "* Compute type: default\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-04-12 19:30:59.304] [ctranslate2] [thread 687705] [warning] The compute type inferred from the saved model is float16, but the target device or backend do not support efficient float16 computation. The model weights have been automatically converted to use the float32 compute type instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No language specified, language will be first be detected for each audio file (increases inference time).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.2.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../../../../.cache/torch/whisperx-vad-segmentation.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.2.1. Bad things might happen unless you revert torch to 1.x.\n",
      ">>>>>> Now, whisper is working on salma_hayek_short.mp4\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "### Load the whisper model\n",
    "# set the device, batch size, and compute type\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # setting device on GPU if available, else CPU\n",
    "batch_size = 16 if device == \"cuda\" else 4 # reduce to 4 if low on GPU memory\n",
    "compute_type = \"float16\" if device == \"cuda\" else \"default\"\n",
    "model_size = \"base\" # change to \"large-v3\" for better performance (note that it is significantly slower)\n",
    "print(f\"* Using device: {device} \\n* Batch size: {batch_size} \\n* Model size: {model_size} \\n* Compute type: {compute_type}\")\n",
    "\n",
    "# load model from whisper\n",
    "model = whisperx.load_model(model_size, device, compute_type=compute_type)\n",
    "\n",
    "\n",
    "### Transcribe all audio/video files in the input folder\n",
    "# iterate over files in the videos folder & apply whisper model on each videos\n",
    "for filename in os.listdir(input_folder):\n",
    "    path = os.path.join(input_folder, filename)\n",
    "\n",
    "    # check if it is a wav file\n",
    "    if filename.endswith(\".wav\") or filename.endswith(\".mp4\"):\n",
    "        # check if the output file already exists\n",
    "        output_filename = filename.split(\".\")[0] + \".txt\"\n",
    "        if os.path.exists(os.path.join(output_folder, output_filename)):\n",
    "            print(f\"{output_filename} already exists in the output folder\")\n",
    "        else:\n",
    "            #apply whisper model on each file\n",
    "            print(\">>>>>> Now, whisper is working on \" + filename)\n",
    "\n",
    "            # 1. Transcribe with original whisper (batched)\n",
    "            audio = whisperx.load_audio(path)\n",
    "            result = model.transcribe(audio, batch_size=batch_size)\n",
    "            print(\">>>>>> Transcription complete. Now aligning the text with the audio...\")\n",
    "\n",
    "            # 2. Align whisper output\n",
    "            model_a, metadata = whisperx.load_align_model(language_code=result[\"language\"], device=device)\n",
    "            result = whisperx.align(result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n",
    "\n",
    "            #export the results as tsv\n",
    "            # export_transcript_as_tsv(result, output_filename, output_folder)\n",
    "            export_transcript_as_tsv_textonly(result, output_filename, output_folder)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">Exercise 1: Import transcript to ELAN</font>\n",
    "Let's import the transcript to ELAN. [Here](https://www.mpi.nl/corpus/html/elan/ch04s03s01.html#Sec_Importing_CSV_Tab-delimited_Text_Files)'s official documentation of ELAN for importing csv files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">Exercise 2: Change the model size</font>\n",
    "Change the model_size in the code above to \"large-v2\" and run the Whisper model again. After running the model, answer the following questions:\n",
    "\n",
    "- Is the output more accurate compared to the based model?\n",
    "- How long did it take for the large-v2 model to process a 37 seconds video?\n",
    "- Did audience's voice affect the transcription accuracy?\n",
    "\n",
    "*Make sure to add \"_base\" to the filename in the output folder. This is because whisper model won't run if there's a csv file with the same filename as the input wav file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sho",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
